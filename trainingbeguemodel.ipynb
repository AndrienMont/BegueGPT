{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"b4477a0d55d8390c","cell_type":"code","source":"!pip install -q -U accelerate datasets peft transformers trl wandb bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:47:54.644814Z","iopub.execute_input":"2025-02-11T13:47:54.645104Z","iopub.status.idle":"2025-02-11T13:48:23.822469Z","shell.execute_reply.started":"2025-02-11T13:47:54.645081Z","shell.execute_reply":"2025-02-11T13:48:23.821583Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.9/313.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"id":"6ec4266ecdfecbc0","cell_type":"code","source":"\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n\n# Specify the checkpoint for SmolLM2 and set the device.\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n\n# Load the tokenizer and model.\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n# For multi-GPU setups, consider using device_map=\"auto\":\nmodel = AutoModelForCausalLM.from_pretrained(\n        checkpoint,\n        device_map=\"auto\",  # {\"\": PartialState().process_index}\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:48:30.367195Z","iopub.execute_input":"2025-02-11T13:48:30.367674Z","iopub.status.idle":"2025-02-11T13:49:04.072676Z","shell.execute_reply.started":"2025-02-11T13:48:30.367612Z","shell.execute_reply":"2025-02-11T13:49:04.071962Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afbe901171a445908d2ee522c3a2d1ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ea636edfa4f43b8939c20836b3f42ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"173419e610a141d7b17716fa7b18b5a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5616dd7181e47d099d74cea16b650ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc75efd320bc4c65b36219b40302cd0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9c10c2e13794b8697f347f8a90b151b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f794e85f500a46d7b71fc510e0d2cafc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a22240b78f1143d68e614f03fbb811fb"}},"metadata":{}}],"execution_count":2},{"id":"4cf9e398a3a33cc9","cell_type":"code","source":"tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ff7fd8071c7f156c","cell_type":"code","source":"model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"37633c01c195e6c9","cell_type":"markdown","source":"# Dataset\n\nJson structure output: https://huggingface.co/datasets/ChristianAzinn/json-training","metadata":{}},{"id":"9dd463d66d84fae8","cell_type":"code","source":"from datasets import load_dataset\n\ndataset_name = \"Khmarigou/Begue\"\nds = load_dataset(\"Khmarigou/alpace_begue_fr\")\n# Perform Train-Test Split\nsplit_ds = ds[\"train\"].train_test_split(test_size=0.2, seed=42)\n\n# Access train and test splits\ntrain_dataset = split_ds[\"train\"].select(range(50))\ntest_dataset = split_ds[\"test\"].select(range(1000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T15:46:33.138648Z","iopub.execute_input":"2025-02-11T15:46:33.138964Z","iopub.status.idle":"2025-02-11T15:46:34.831109Z","shell.execute_reply.started":"2025-02-11T15:46:33.138938Z","shell.execute_reply":"2025-02-11T15:46:34.830395Z"}},"outputs":[],"execution_count":22},{"id":"8ceed737fd63f0f4","cell_type":"code","source":"train_dataset\ntest_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T11:56:32.547288Z","iopub.execute_input":"2025-02-11T11:56:32.547593Z","iopub.status.idle":"2025-02-11T11:56:32.553388Z","shell.execute_reply.started":"2025-02-11T11:56:32.547567Z","shell.execute_reply":"2025-02-11T11:56:32.552579Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['output', 'input', 'instruction'],\n    num_rows: 1000\n})"},"metadata":{}}],"execution_count":20},{"id":"ddf37774132f48c2","cell_type":"code","source":"# Set the response template to match the chat format.\n# (Ensure this string exactly matches the beginning of the assistant's response as output by apply_chat_template.)\nresponse_template = \"<|im_start|>assistant\\n\"\ninstruction_template = \"<|im_start|>user\\n\"\nPROMPT_TEMPLATE = \"\"\"Query: {query}\n\nschema:\n{schema}\"\"\"\n\n\ndef formatting_prompts_func(example):\n    \"\"\"\n    Converts each example into a conversation string using the tokenizer's chat template.\n    Assumes each example contains lists under \"instruction\" and \"output\".\n    \"\"\"\n    output_texts = []\n    for i in range(len(example[\"instruction\"])):\n        # Build a conversation with a user message and an assistant reply.\n        messages = [\n            {\n                \"role\":    \"system\",\n                \"content\": \"You are a person who stutter.\"\n                },\n            {\"role\": \"user\", \"content\": example['instruction'][i]},\n            # Note: It is important that the assistant message content here does not\n            # include the assistant marker, because the chat template will insert it.\n            {\"role\": \"assistant\", \"content\": example[\"output\"][i]}\n            ]\n        # Use the chat template to generate the formatted text.\n        text = tokenizer.apply_chat_template(messages, tokenize=False)\n        output_texts.append(text)\n    return output_texts\n\n\n# Create the data collator.\n# It will search for the response_template (here \"Assistant:\") in the formatted text\n# and ensure that only tokens after that marker contribute to the loss.\ncollator = DataCollatorForCompletionOnlyLM(response_template=response_template,\n                                           instruction_template=instruction_template,\n                                           tokenizer=tokenizer,\n                                           mlm=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:49:14.316729Z","iopub.execute_input":"2025-02-11T13:49:14.317018Z","iopub.status.idle":"2025-02-11T13:49:14.328967Z","shell.execute_reply.started":"2025-02-11T13:49:14.316996Z","shell.execute_reply":"2025-02-11T13:49:14.328182Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/trl/trainer/utils.py:112: UserWarning: The pad_token_id and eos_token_id values of this tokenizer are identical. If you are planning for multi-turn training, it can result in the model continuously generating questions and answers without eos token. To avoid this, set the pad_token_id to a different value.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"id":"793ac796b6f1284c","cell_type":"code","source":"tokenizer.apply_chat_template([\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n    {\"role\": \"assistant\", \"content\": \"I am good, thank you.\"}\n    ], tokenize=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:11:56.473975Z","iopub.execute_input":"2025-02-11T13:11:56.474359Z","iopub.status.idle":"2025-02-11T13:11:56.490626Z","shell.execute_reply.started":"2025-02-11T13:11:56.474326Z","shell.execute_reply":"2025-02-11T13:11:56.489090Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nHello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI am good, thank you.<|im_end|>\\n'"},"metadata":{}}],"execution_count":5},{"id":"a96feb2a6e00d710","cell_type":"markdown","source":"# Lora Config","metadata":{}},{"id":"da1f50aa45ee897a","cell_type":"code","source":"from peft import LoraConfig\n\n# Note that r, in the figure above, is a hyperparameter here that we can use to specify the rank of the low-rank matrices used for adaptation.\n# A smaller r leads to a simpler low-rank matrix, which results in fewer parameters to learn during adaptation.\n# This can lead to faster training and potentially reduced computational requirements.\n# However, with a smaller r, the capacity of the low-rank matrix to capture task-specific information decreases.\n# This may result in lower adaptation quality, and the model might not perform as well on the new task compared to a higher r.\nlora_config = LoraConfig(\n        r=32,\n        lora_alpha=64,\n        lora_dropout=0.05,\n        target_modules=['o_proj', 'k_proj', 'q_proj', \"v_proj\"],\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:49:20.687759Z","iopub.execute_input":"2025-02-11T13:49:20.688082Z","iopub.status.idle":"2025-02-11T13:49:20.692081Z","shell.execute_reply.started":"2025-02-11T13:49:20.688055Z","shell.execute_reply":"2025-02-11T13:49:20.691272Z"}},"outputs":[],"execution_count":5},{"id":"7c2d2ea451fe4ba0","cell_type":"markdown","source":"# Wandb\n\nCreat token and account: https://wandb.ai/home","metadata":{}},{"id":"7c2ddcdf716fe186","cell_type":"code","source":"import wandb\nimport getpass\n\ntoken = getpass.getpass()\nwandb.login(key=token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:49:26.592729Z","iopub.execute_input":"2025-02-11T13:49:26.593028Z","iopub.status.idle":"2025-02-11T13:49:37.731024Z","shell.execute_reply.started":"2025-02-11T13:49:26.593004Z","shell.execute_reply":"2025-02-11T13:49:37.730322Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":" ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33memilien-boitouzet-cours\u001b[0m (\u001b[33memilien-boitouzet-cours-usmb\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":6},{"id":"5641c6e8-123f-4056-a290-40e6d98a4492","cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:49:38.205766Z","iopub.execute_input":"2025-02-11T13:49:38.206623Z","iopub.status.idle":"2025-02-11T13:49:38.229008Z","shell.execute_reply.started":"2025-02-11T13:49:38.206570Z","shell.execute_reply":"2025-02-11T13:49:38.228163Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a63e5625af10450a8ac8acecad6ecb6e"}},"metadata":{}}],"execution_count":7},{"id":"da461c98-7113-4054-bb9f-c6a56ab0c1e2","cell_type":"code","source":"hub_model_id = \"Khmarigou/Begue\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T13:49:47.051694Z","iopub.execute_input":"2025-02-11T13:49:47.051998Z","iopub.status.idle":"2025-02-11T13:49:47.055809Z","shell.execute_reply.started":"2025-02-11T13:49:47.051975Z","shell.execute_reply":"2025-02-11T13:49:47.054948Z"}},"outputs":[],"execution_count":8},{"id":"cdc523383c666853","cell_type":"markdown","source":"# SFT Trainer config","metadata":{}},{"id":"be4a6a400207624e","cell_type":"code","source":"OUTPUT_DIR = checkpoint.split(\"/\")[-1] + \"-structure-output\"\n\n# setup the trainer\ntrainer = SFTTrainer(\n        model=model,\n        train_dataset=train_dataset,\n        eval_dataset=test_dataset,\n        args=SFTConfig(\n                per_device_train_batch_size=2,\n                gradient_accumulation_steps=4,\n                warmup_steps=100,\n                num_train_epochs=3,\n                learning_rate=0.00002,\n                lr_scheduler_type=\"cosine\",\n                eval_strategy=\"steps\",\n                eval_steps=500,\n                weight_decay=0.01,\n                bf16=True,\n                logging_strategy=\"steps\",\n                logging_steps=10,\n                output_dir=\"./\" + OUTPUT_DIR,\n                optim=\"paged_adamw_8bit\",\n                seed=42,\n                run_name=f\"train-{OUTPUT_DIR}\",\n                report_to=\"wandb\",\n                save_steps=31,\n                push_to_hub=True,\n                hub_model_id=hub_model_id,\n                save_total_limit=4\n                ),\n        peft_config=lora_config,\n        formatting_func=formatting_prompts_func,\n        data_collator=collator,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T15:46:42.725398Z","iopub.execute_input":"2025-02-11T15:46:42.725755Z","iopub.status.idle":"2025-02-11T15:46:43.705471Z","shell.execute_reply.started":"2025-02-11T15:46:42.725724Z","shell.execute_reply":"2025-02-11T15:46:43.704820Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac5bb36d1b4c432aaabdff82ab84de64"}},"metadata":{}}],"execution_count":23},{"id":"d97b2cd2954f98af","cell_type":"code","source":"import os\nfrom transformers import is_torch_xpu_available, is_torch_npu_available\nimport torch\n\n# Lancement du processus d'entraînement du modèle.\n# Ici, 'trainer.train()' déclenche la phase de fine-tuning,\n# dans laquelle les paramètres du modèle sont ajustés sur une tâche spécifique\n# en utilisant des données d'entraînement pertinentes.\ntrainer.train()\n\n# Une fois l'entraînement terminé, on sauvegarde l'adaptateur LoRA (fine-tuning léger).\n# LoRA (Low-Rank Adaptation) est une technique destinée à fine-tuner les grands\n# modèles en modifiant uniquement un sous-ensemble restreint de paramètres.\nfinal_checkpoint_dir = os.path.join(OUTPUT_DIR, \"final_checkpoint\")\ntrainer.save_model(final_checkpoint_dir)\n\ntrainer.push_to_hub(dataset_name=dataset_name)\n\n# Nettoyage des ressources mémoire pour libérer l'espace GPU ou autres accélérateurs,\n# ce qui est utile avant de fusionner l'adaptateur LoRA avec le modèle de base.\ndel model  # Suppression explicite du modèle de la mémoire.\n\n# Vider les caches des accélérateurs (XPU, NPU ou GPU en fonction de la disponibilité).\n# Cela optimise l'utilisation future des ressources.\nif is_torch_xpu_available():\n    torch.xpu.empty_cache()  # Vide les caches spécifiques pour XPU.\nelif is_torch_npu_available():\n    torch.npu.empty_cache()  # Vide les caches spécifiques pour NPU.\nelse:\n    torch.cuda.empty_cache()  # Vide les caches GPU standard.\n\n# Chargement du modèle adapté (en incluant l'adaptateur LoRA) pour effectuer une fusion\n# avec le modèle de base. Cela permet de sauvegarder un modèle autonome optimisé.\nfrom peft import AutoPeftModelForCausalLM\n\n# Chargement du modèle préalablement sauvegardé depuis le répertoire OUTPUT_DIR.\n# Les paramètres 'device_map' et 'torch_dtype' permettent d'optimiser le chargement :\n# - 'device_map=\"auto\"' ajuste automatiquement le placement sur le GPU, CPU ou autre.\n# - 'torch_dtype=torch.bfloat16' utilise un format numérique bfloat16, qui réduit\n#    la mémoire nécessaire tout en maintenant des performances stables.\nmodel = AutoPeftModelForCausalLM.from_pretrained(\n        OUTPUT_DIR,\n        device_map=\"auto\",\n        torch_dtype=torch.bfloat16\n        )\n\n# Fusion de l'adaptateur LoRA directement dans le modèle de base,\n# afin de produire un modèle final unique tout en réduisant ses redondances.\nmodel = model.merge_and_unload()\n\n# Sauvegarde du modèle fusionné dans un répertoire spécifique.\n# 'safe_serialization=True' garantit que le modèle est stocké au format sûr,\n# pour une compatibilité future et une intégrité des données.\noutput_merged_dir = os.path.join(OUTPUT_DIR, \"final_merged_checkpoint\")\nmodel.save_pretrained(output_merged_dir, safe_serialization=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T15:46:51.207822Z","iopub.execute_input":"2025-02-11T15:46:51.208138Z","iopub.status.idle":"2025-02-11T15:47:44.169617Z","shell.execute_reply.started":"2025-02-11T15:46:51.208114Z","shell.execute_reply":"2025-02-11T15:47:44.168700Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [18/18 00:27, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 548f19b6-7dd5-45e4-96a7-6f9ce5e446ed)') - silently ignoring the lookup for the file config.json in HuggingFaceTB/SmolLM2-135M-Instruct.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:246: UserWarning: Could not find a config file in HuggingFaceTB/SmolLM2-135M-Instruct - will assume that the vocabulary was not modified.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":24},{"id":"f527704d-eeb8-4c3a-be81-eb20bd2e6e2e","cell_type":"code","source":"model.push_to_hub(hub_model_id)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:33:17.557683Z","iopub.execute_input":"2025-02-11T14:33:17.557971Z","iopub.status.idle":"2025-02-11T14:33:33.639622Z","shell.execute_reply.started":"2025-02-11T14:33:17.557948Z","shell.execute_reply":"2025-02-11T14:33:33.638918Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8daac17d4fb4f00a185817651d82bd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3b9ccf8ebff449fa53b70900da9ec71"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Khmarigou/Begue/commit/8096f0a7ecd9bb1999b0a7c5072c5f342b0c2f9d', commit_message='Upload LlamaForCausalLM', commit_description='', oid='8096f0a7ecd9bb1999b0a7c5072c5f342b0c2f9d', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Khmarigou/Begue', endpoint='https://huggingface.co', repo_type='model', repo_id='Khmarigou/Begue'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":11},{"id":"22753b825ec99281","cell_type":"markdown","source":"# inference","metadata":{}},{"id":"3be83d7a3fbeed60","cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\"  # for GPU usage or \"cpu\" for CPU usage\ntokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\nmodel = AutoModelForCausalLM.from_pretrained(OUTPUT_DIR).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T15:47:56.142282Z","iopub.execute_input":"2025-02-11T15:47:56.142668Z","iopub.status.idle":"2025-02-11T15:47:57.013103Z","shell.execute_reply.started":"2025-02-11T15:47:56.142634Z","shell.execute_reply":"2025-02-11T15:47:57.012408Z"}},"outputs":[],"execution_count":25},{"id":"f7429862c409ccdf","cell_type":"code","source":"messages = [\n            {\n                \"role\":    \"system\",\n                \"content\": \"You are a person who stutter.\"\n                },\n    {\n        \"role\":    \"user\",\n        \"content\": \"Tell me a story.\"\n        },\n    ]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T14:45:45.689522Z","iopub.execute_input":"2025-02-11T14:45:45.689851Z","iopub.status.idle":"2025-02-11T14:45:45.694225Z","shell.execute_reply.started":"2025-02-11T14:45:45.689825Z","shell.execute_reply":"2025-02-11T14:45:45.693425Z"}},"outputs":[],"execution_count":15},{"id":"ebdddc84e21bdfdf","cell_type":"code","source":"input_text = tokenizer.apply_chat_template(messages, tokenize=False)\nprint(input_text)\nprint(\"----------------- Generated text -----------------\")\ninputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\noutputs = model.generate(inputs,\n                         max_new_tokens=1000,\n                         temperature=0.2,\n                         top_p=0.45,\n                         eos_token_id=tokenizer.eos_token_id,   # <-- crucial\n                         pad_token_id=tokenizer.eos_token_id,    # often set pad = eos\n                         do_sample=True)\nprint(tokenizer.decode(outputs[0]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T15:47:59.105462Z","iopub.execute_input":"2025-02-11T15:47:59.105822Z","iopub.status.idle":"2025-02-11T15:48:45.416942Z","shell.execute_reply.started":"2025-02-11T15:47:59.105795Z","shell.execute_reply":"2025-02-11T15:48:45.415943Z"}},"outputs":[{"name":"stdout","text":"<|im_start|>system\nYou are a person who stutter.<|im_end|>\n<|im_start|>user\nTell me a story.<|im_end|>\n\n----------------- Generated text -----------------\n<|im_start|>system\nYou are a person who stutter.<|im_end|>\n<|im_start|>user\nTell me a story.<|im_end|>\n<|im_start|>assistant\nI've got a story brewing. It's a tale of a young woman named Maya, who's been living in a small, isolated town on the outskirts of the city. She's a talented artist, but her work is often dismissed by the locals as \"talentless\" or \"unrealistic.\" Despite her talents, she's been struggling to make ends meet, and the town's poverty is taking a toll on her mental health.\n\nOne day, a mysterious stranger arrives in town, dressed in a long coat and carrying a small, ornate box. The stranger introduces himself as \"The Architect,\" and he's a master of the art of architecture. He's been working on a project for years, but he's been unable to complete it due to a series of unforeseen circumstances.\n\nThe Architect arrives at Maya's doorstep, and she's taken aback by his presence. He's a tall, imposing figure with a kind face and a kind heart. He introduces himself as \"Echo,\" and he's a master of the art of manipulation. Echo is a master of the art of persuasion, and he's been using his skills to manipulate people, often to get what he wants.\n\nMaya is taken aback by Echo's presence, and she's unsure of what to do. She's tried to talk to Echo, but he's not receptive to conversation. She's tried to talk to Echo, but he's not receptive to conversation either. She's tried to talk to Echo, but he's not receptive to conversation either.\n\nAs the days go by, Maya starts to feel like she's losing her mind. She's struggling to make ends meet, and she's starting to feel like she's losing herself. She's starting to wonder if she's truly alive or if she's just a puppet on a string.\n\nOne day, Maya meets a mysterious figure who seems to be a guide for Echo. Maya is intrigued by the figure's appearance and the fact that she's dressed in a long coat. Maya is intrigued by the figure's appearance and the fact that he's dressed in a long coat. Maya is intrigued by the figure's appearance and the fact that he's dressed in a long coat.\n\nAs Maya meets the figure, she's struck by his presence. She's struck by his presence, and she's struck by the fact that he's dressed in a long coat. Maya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat.\n\nMaya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat. Maya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat.\n\nMaya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat. Maya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat.\n\nMaya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat. Maya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat.\n\nMaya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat. Maya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat.\n\nMaya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat. Maya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat.\n\nMaya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat. Maya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat.\n\nMaya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat. Maya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he's dressed in a long coat.\n\nMaya is struck by the fact that he's dressed in a long coat, and she's struck by the fact that he\n","output_type":"stream"}],"execution_count":26}]}